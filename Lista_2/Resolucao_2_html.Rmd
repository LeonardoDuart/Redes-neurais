---
title: "Resolução da lista 2:Dropout e Keras"
output:
  rmdformats::downcute:
    code_folding: show
    df_print: paged
    self_contained: true
    thumbnails: true
    lightbox: true
    gallery: false
    highlight: tango
---
  
```{=html}
<style>
body {
text-align: left}
</style>
```

```{r steupp, out.width = "400px",echo=FALSE}
htmltools::img(src = knitr::image_uri("D:/UNB/9_semestre/Redes_neurais/Lista_1/logo-est.png"), 
               alt = 'logo', 
               style = 'position:absolute; top:0; right:0; padding:10px;')
```


```{r real_setup,  error=FALSE, echo =FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE)
options(scipen = 9999)

pdf_link <- htmltools::a("Download da lista 2", href = "D:/UNB/9_semestre/Redes_neurais/Lista_2/Lista_2.pdf")

load("D:/UNB/9_semestre/Redes_neurais/Lista_2/Bench.RData")

  # Para salvar
# save_model_tf(mod, "D:/UNB/9_semestre/Redes_neurais/Lista_2/modelos/mod")
# save_model_tf(mod_b, "D:/UNB/9_semestre/Redes_neurais/Lista_2/modelos/mod_b")
# save_model_tf(mod_f, "D:/UNB/9_semestre/Redes_neurais/Lista_2/modelos/mod_f")

  # Para importar
mod <- keras::load_model_tf('D:/UNB/9_semestre/Redes_neurais/Lista_2/modelos/mod')
mod_b <- keras::load_model_tf('D:/UNB/9_semestre/Redes_neurais/Lista_2/modelos/mod_b')
mod_f <- keras::load_model_tf('D:/UNB/9_semestre/Redes_neurais/Lista_2/modelos/mod_f')
```
# **Dados do aluno**

Nome: Leonardo Gomes Duart

Matrícula: 190016183

E-mail: leonardo.duart@hotmail.com

# **Introdução e códigos do professor**

Para acessar a lista completa, basta clicar no botão abaixo descrito como "Download da lista 2":
```{r real_setup_2,  error=FALSE, echo =FALSE}
pdf_link
```


```{r func_escondida,  error=FALSE, echo =FALSE}
    sigmoide <- function(x) {
          return(1/(1+exp(-x)))}
    
    forward_prop <- function(theta, x) {
      ifelse(is.double(x), x <- as.matrix(x), x <- t(as.matrix(x)))
      W1 <- matrix(data = theta[1:4], nrow = 2)
      W2 <- matrix(data = theta[5:6], nrow = 2)
      b1 <- theta[7:8]
      b2 <- theta[9]
      a <- matrix(data = rep(b1, ncol(x)), nrow = 2) + W1 %*% x
      h <- sigmoide(a)
      y_hat <- as.double(b2 + t(W2) %*% h)
      return(y_hat)}
    
    mse_cost <- function(y_true, y_hat) {
      return(mean((y_true - y_hat)^2))}

    back_prop <- function(theta, x, y){
      ifelse(is.double(x), x <- as.matrix(x), x <- t(as.matrix(x)))
      W1 <- matrix(data = theta[1:4], nrow = 2)
      W2 <- matrix(data = theta[5:6], nrow = 2)
      b1 <- theta[7:8]
      b2 <- theta[9]
      a <- matrix(data = rep(b1, ncol(x)), nrow = 2) + W1 %*% x
      h <- sigmoide(a)
      y_hat <- as.double(b2 + t(W2) %*% h)
      g <- -2*(y - y_hat)/length(y)
      grad_b2 <- sum(g)
      grad_W2 <- g %*% t(h)
      g <- W2 %*% g
      g <- g * derivada_sigmoide(a)
      grad_b1 <- rowSums(g)
      grad_W1 <- g %*% t(x)
      g <- W1 %*% g
      vetor_grad <- c(grad_W1, grad_W2, grad_b1, grad_b2)
      names(vetor_grad) <- c(paste0("w", 1:6), paste0("b", 1:3))
      return(vetor_grad)}
    
    derivada_sigmoide <- function(x) {
return(exp(-x)/((1+exp(-x))^2))}
```

Para a resolução da lista foram utilizados os pacotes abaixo:

```{r setup, error=FALSE}
pacman::p_load("tidyverse","ggplot2","ggpubr","microbenchmark","plotly","keras",
               "tensorflow")

  # Setup para o keras funcionar
# install.packages(c('keras','tensorflow'))
# install.packages('devtools')
# devtools::install.github('rstudio/keras', dependencies = T)
# devtools::install.github('rstudio/tensorflow', dependencies = T)
# install_keras()
# install_tensorflow()
```

Abaixo temos o modelo de foco, como disponibilizado pelo professor:

```{r cache=T}
### Gerando dados "observados"
set.seed(1.2023)
m.obs <- 100000
dados <- tibble(x1.obs=runif(m.obs, -3, 3), 
                x2.obs=runif(m.obs, -3, 3)) %>%
  mutate(mu=abs(x1.obs^3 - 30*sin(x2.obs) + 10), 
         y=rnorm(m.obs, mean=mu, sd=1))

  # Treino
treino <- dados[1:80000,]
x1.treino <- treino$x1.obs
x2.treino <- treino$x2.obs
x.treino <- treino %>%
select(x1.obs, x2.obs)
y.treino <- treino$y

  # Teste
teste <- dados[80001:100000,]
x1.teste <- teste$x1.obs
x2.teste <- teste$x2.obs
x.teste <- teste %>%
select(x1.obs, x2.obs)
y.teste <- teste$y
```

# **Questão 1**

## **Letra A**

**a)** Altere seu código da Lista 1 (ou, se preferir, os códigos disponibilizados como gabarito) para implementar a técnica dropout na camada de entrada e na camada intermediária. Use $p = 0,6$, onde p representa a probabilidade de inclusão de cada neurônio. Reporte o custo dessa rede.

Vou utilizar o código do gabarito para mais organização.

```{r 1a,  error=FALSE, cache=TRUE, warning=FALSE}
  # Vou mudar a função foward prop para adaptar ao dropout
forward_prop_drop <- function(theta, x, dropout_rate) {
  ifelse(is.double(x), x <- as.matrix(x), x <- t(as.matrix(x)))
  
    # Dropout na camada de entrada
  x <- sweep(x, 2, rbinom(n = length(x), size = 1, prob = dropout_rate), "*")
  
  W1 <- matrix(data = theta[1:4], nrow = 2)
  W2 <- matrix(data = theta[5:6], nrow = 2)
  b1 <- theta[7:8]
  b2 <- theta[9]
  
    # Calculo na camada oculta
  a <- matrix(data = rep(b1, ncol(x)), nrow = 2) + W1 %*% x
  
  h <- sigmoide(a)
  
    # Dropout na camada intermediária
  h <- sweep(h, 2, rbinom(n = length(h), size = 1, prob = dropout_rate), "*")
  
  y_hat <- as.double(b2 + t(W2) %*% h)
  
  return(y_hat)
}

  # Vou mudar a função back prop para adaptar ao dropout
back_prop_drop <- function(theta, x, y, dropout_rate) {
  
    # Primeiro, deve-se realizar o forward propagation
  ifelse(is.double(x), x <- as.matrix(x), x <- t(as.matrix(x)))
  W1 <- matrix(data = theta[1:4], nrow = 2)
  W2 <- matrix(data = theta[5:6], nrow = 2)
  b1 <- theta[7:8]
  b2 <- theta[9]
  
    # Camada de entrada com dropout
  x <- sweep(x, 2, rbinom(n = length(x), size = 1, prob = dropout_rate), "*")
  
    # Calculo na camada oculta
  a <- matrix(data = rep(b1, ncol(x)), nrow = 2) + W1 %*% x

  h <- sigmoide(a)
    
    # Camada escondida com dropout
  h <- sweep(h, 2, rbinom(n = length(h), size = 1, prob = dropout_rate), "*")
  
  y_hat <- as.double(b2 + t(W2) %*% h)
  
  g <- -2*(y - y_hat)/length(y)
  grad_b2 <- sum(g)
  grad_W2 <- g %*% t(h)
  g <- W2 %*% g
  g <- g * derivada_sigmoide(a)
  grad_b1 <- rowSums(g)
  grad_W1 <- g %*% t(x)
  g <- W1 %*% g
  
    # Criamos um vetor com os gradientes de cada parâmetro
  vetor_grad <- c(grad_W1, grad_W2, grad_b1, grad_b2)
  names(vetor_grad) <- c(paste0("w", 1:6), paste0("b", 1:3))
  return(vetor_grad)
}

  # Epsilon, iterações, theta da lista anterior letra e, p_dropout 0.6
theta_est <- list()
theta_est[[1]] <- rep(0, 9)
p_dropout = 0.6
epsilon <- 0.1
M <- 100

  # Vetores para receber os custos
custo_treino <- custo_teste <- numeric(M)  
  
  # Semente
set.seed(123)

  # Execução
for(i in 1:M) {
    # Cálculo dos gradientes dos parâmetros
  grad <- back_prop_drop(theta_est[[i]],
                    x.treino,y.treino,
                    p_dropout)
    # Cálculo do custo de treino
  custo_treino[i] <- mse_cost(y.treino, forward_prop_drop(theta_est[[i]], 
                                                     x.treino,
                                                     p_dropout))
    # Cálculo do custo de teste
  custo_teste[i] <- mse_cost(y.teste, forward_prop_drop(theta_est[[i]], 
                                                     x.treino,
                                                     p_dropout))
    # Atualização dos parâmetros
  theta_est[[i+1]] <- theta_est[[i]] - epsilon*grad
  }

  # Respostas
cat('Menor custo banco de treino \n',
    round(min(custo_treino), 3),'\n',
    'Ocorreu na iteração \n',
    which.min(custo_treino))

cat('Menor custo banco de teste \n',
    round(min(custo_teste), 3),'\n',
    'Ocorreu na iteração \n',
    which.min(custo_teste))

min_theta <- unlist(theta_est[which.min(custo_teste)])

cat('Vetor de pesos que minimizou o banco de teste \n',
    round(min_theta, 3))
```
## **Letra B**

**b)** Considerando os pesos obtidos em a), para a primeira observação do conjunto de teste, gere 200 previsões ($y_{1,1}$,..., $y_{1,200}$), uma para cada sub-rede amostrada aleatoriamente. Use as previsões para construir uma estimativa pontual e um intervalo de confiança para y1. Veja a Figura 7.6 do livro Deep Learning. Note que com esse procedimento, não é preciso assumir normalidade para os erros, como
fizemos na Lista 1.

```{r 1b,  error=FALSE, cache=TRUE,warning=FALSE}
  # Semente
set.seed(123)  

  # Número de sub-redes amostradas
n_samples <- 200  

  # Vetor para armazenar as previsões
y_hat_sample <- numeric(n_samples)  

  # Gerar as previsões para as sub-redes amostradas
for (i in 1:n_samples) {
  y_hat_sample[i] <- forward_prop_drop(theta = theta_est[[which.min(custo_teste)]],
                                           x = x.teste[1, ], dropout_rate = 0.6)
}

  # Estimativa pontual
y_hat_mean <- mean(y_hat_sample)

  # Intervalo de confiança
alpha <- 0.05  # Nível de confiança (95% de confiança)

lower <- quantile(y_hat_sample, alpha/2)
upper <- quantile(y_hat_sample, 1 - alpha/2)

  # Imprimir estimativa pontual e intervalo de confiança
cat("Estimativa pontual:", y_hat_mean, "\n",
    "Intervalo de confiança (", (1-alpha)*100, "%): [", lower, ", ", upper, "]", "\n")
```

## **Letra C**

**c)** Repita o item b) para gerar estimativas pontuais para cada observação do conjunto de testes.

```{r 1c,  error=FALSE, cache=TRUE, warning=FALSE}
  # Semente
set.seed(123)  

  # Número de sub-redes amostradas
n_samples <- 200  

  # Matriz para armazenar as previsões
y_hat_samples <- matrix(NA, nrow = n_samples, ncol = nrow(x.teste))

# Gerar as previsões para cada observação do conjunto de testes
for (i in 1:n_samples) {
  y_hat_samples[i, ] <- forward_prop_drop(theta = theta_est[[which.min(custo_teste)]],
                                          x = x.teste, dropout_rate = 0.6)
}

# Estimativas pontuais para cada observação
y_hat_mean <- colMeans(y_hat_samples)

# Imprimir estimativas pontuais para cada observação
cat("Primeiras 10 estimativas pontuais para cada observação:\n",
    head(y_hat_mean,10))
```

## **Letra D**

**d)** Use a regra weight scaling inference rule (página 263 do livro Deep Learning) para gerar novas estimativas para as observações do conjunto de testes. Qual dos procedimentos (o do item c) ou o utilizado neste item) produziu melhores resultados? Considerando o tempo computacional de cada um, qual você escolheria nessa aplicação?

```{r 1d,  error=FALSE, cache=TRUE}

```

# **Questão 2**

## **Letra A**

**a)** Ajuste a rede neural especificada na Lista 1 usando o *Keras*. Compare com sua implementação (Lista 1, item e) quanto ao tempo computacional e ao custo obtido no conjunto de teste. Use o mesmo algoritmo de otimização (*full gradient descent*) e ponto de partida.

```{r 2a,  error=FALSE, cache=TRUE, eval=FALSE}
  # ponto de partida
theta <- rep(0,9)

  # Criando o modelo
mod <- keras_model_sequential()

  # Criando camadas do modelo
mod %>% 
  layer_dense(input_shape=2, # entra x1 e x2
              units=2, # sai h1 e h2
              activation = 'sigmoid') %>% 
  layer_dense(input_shape=2, # entra h1 e h2
              units=1) # sai yi

  # Ajustando os pesos com base no vetor theta
# Deve-se definir os bias como array para o keras aceitar
weights_layer1 <- list(matrix(theta[1:4], ncol = 2), # w1,w2,w3,w4
                       array(matrix(theta[5:6], ncol = 2))) # b1,b2

weights_layer2 <- list(matrix(theta[7:8], ncol = 1), # w5,w6
                       array(theta[9])) # b3
# Definindo de fato
mod$layers[[1]]$set_weights(weights_layer1)
mod$layers[[2]]$set_weights(weights_layer2)

  # Tx de aprendizagem 0.1 e funcao de perda mse
tx_aprendizagem = 0.1

mod %>% 
  compile(optimizer=optimizer_sgd( # sgd =  gradiente descendente estocastico
    learning_rate = tx_aprendizagem),
          loss="mse")

  # Modelo na pratica com o callback para salvar o ponto otimizado
mod %>% 
  fit(x=matrix(c(x1.treino, x2.treino), nc=2),
      y=y.treino,
      validation_data=list(matrix(c(x1.teste, x2.teste), nc=2),
                           y.teste),
      callbacks=callback_early_stopping(
        monitor = "val_loss",
        patience = 10, 
        restore_best_weights = T),
      batch_size=length(treino$y),
      epochs=500)
```

```{r 2apt2.1,  error=FALSE, cache=TRUE, message=TRUE}
  # Resultado
mod %>% 
  evaluate(x = matrix(c(x1.teste, x2.teste), nc = 2), y = y.teste) -> mod_resultado

mod_resultado
```

Acima temos o custo obtido no conjunto de teste. Também é igual ao da lista 1 questão e).

Agora fazer um cálculo de eficiência com o pacote *microbenchmark* para comparamos: **com o early stopping**, **sem o early stopping** e **manualmente como na lista 1 letra e)**, com 100 *epochs* (quantidade de iterações na lista 1). Vale ressaltar que a placa de vídeo utilizada é da Nvidia, modelo RTX 2060.

```{r 2apt2,  error=FALSE, cache=TRUE, eval= FALSE}
microbenchmark::microbenchmark(
  
  #metodo lista1
  manual = {
    corte <- 80000
    treino <- dados[1:corte,]
    teste <- dados[(corte+1):nrow(dados),]
    x_treino <- treino %>%
    select(x1.obs, x2.obs)
    x_teste <- teste %>%
    select(x1.obs, x2.obs)
    y_treino <- treino$y
    y_teste <- teste$y
    theta_est <- list()
    theta_est[[1]] <- rep(0, 9)
    epsilon <- 0.1
    M <- 100
    custo_treino <- custo_teste <- numeric(M)

    for(i in 1:M) {
    grad <- back_prop(theta = theta_est[[i]], x = x_treino, y = y_treino)
    custo_treino[i] <- mse_cost(y_treino, forward_prop(theta_est[[i]], x_treino))
    custo_teste[i] <- mse_cost(y_teste, forward_prop(theta_est[[i]], x_teste))
    theta_est[[i+1]] <- theta_est[[i]] - epsilon*grad
}
  },
  
  #metodo lista sem early stopping
  keras_no_callback = {  
    theta <- rep(0,9)

    mod_bench <- keras_model_sequential()

    mod_bench %>% 
      layer_dense(input_shape=2,units=2,activation = 'sigmoid') %>% 
      layer_dense(input_shape=2,units=1) 

    weights_layer1 <- list(matrix(theta[1:4], ncol = 2), 
                       array(matrix(theta[5:6], ncol = 2)))

    weights_layer2 <- list(matrix(theta[7:8], ncol = 1),
                       array(theta[9]))

    mod_bench$layers[[1]]$set_weights(weights_layer1)
    mod_bench$layers[[2]]$set_weights(weights_layer2)

    tx_aprendizagem = 0.1

    mod_bench %>% 
      compile(optimizer=optimizer_sgd(learning_rate = tx_aprendizagem),loss="mse") %>% 
      fit(x=matrix(c(x1.treino, x2.treino), nc=2), y=y.treino,
          validation_data=list(matrix(c(x1.teste, x2.teste), nc=2),
                           y.teste),
          batch_size=length(treino$y),
          epochs=100)
},

  #metodo lista com early stopping
  keras_callback = {  
    theta <- rep(0,9)

    mod_bench <- keras_model_sequential()

    mod_bench %>% 
      layer_dense(input_shape=2,units=2,activation = 'sigmoid') %>% 
      layer_dense(input_shape=2,units=1) 

    weights_layer1 <- list(matrix(theta[1:4], ncol = 2), 
                       array(matrix(theta[5:6], ncol = 2)))

    weights_layer2 <- list(matrix(theta[7:8], ncol = 1),
                       array(theta[9]))

    mod_bench$layers[[1]]$set_weights(weights_layer1)
    mod_bench$layers[[2]]$set_weights(weights_layer2)

    tx_aprendizagem = 0.1

    mod_bench %>% 
     compile(optimizer=optimizer_sgd(learning_rate = tx_aprendizagem),loss="mse") %>% 
      fit(x=matrix(c(x1.treino, x2.treino), nc=2), y=y.treino,
          validation_data=list(matrix(c(x1.teste, x2.teste), nc=2),
                           y.teste),
         callbacks=callback_early_stopping(
            monitor = "val_loss",
            patience = 10, 
            restore_best_weights = T),
         batch_size=length(treino$y),
         epochs=100)
},

times = 50) -> bench
```

Abaixo temos então o resultado do benchmark em tabela e gráfico. Pode-se notar que o tempo do modo manual com o keras com callback são parecidos. Porém sem o callback há uma considerável diferença nesse contexto.

```{r 2apt3, error=FALSE,cache=TRUE}
bench %>% summary()

autoplot(bench)
```

## **Letra B**

**b)** Ajuste a rede neural mais precisa (medida pelo MSE calculado sobre o conjunto de validação) que conseguir, com a arquitetura que quiser. Use todos os artifícios de regularização que desejar (*weight decay*, *Bagging*, *droupout*, *Early stopping*). Reporte a precisão obtida.

```{r 2.bpt1, error=FALSE,cache=TRUE, eval=FALSE}
# Criando o modelo
keras_model_sequential() %>%
   layer_dense(units = 256,
               activation = 'relu',
               input_shape = 2) %>%
   layer_dense(units = 128,
               activation = 'relu') %>%
   layer_dense(units = 64,
               activation = 'relu') %>%
   layer_dense(units = 32,
               activation = 'relu') %>%
  layer_dense(units = 8,
               activation = 'softmax') %>%
  layer_dense(units = 8,
               activation = 'relu') %>%
   layer_dense(units = 1)-> mod_b

# Taxa de aprendizagem e função de perda
tx_aprendizagem <- 0.0005

mod_b %>% 
  compile(optimizer = optimizer_adam(learning_rate = tx_aprendizagem),
          loss = "mse")

 # Modelo na pratica com o callback para salvar o ponto otimizado
mod_b %>% 
  fit(x=matrix(c(x1.treino, x2.treino), nc=2),
      y=y.treino,
      validation_data=list(matrix(c(x1.teste, x2.teste), nc=2),
                           y.teste),
      callbacks=callback_early_stopping(
        monitor = "val_loss",
        patience = 25, 
        restore_best_weights = T),
      batch_size=2^7,
      epochs=500)
```



```{r 2.bpt2, error=FALSE, cache=TRUE, message=TRUE}
  # Resultado
mod_b %>% 
  evaluate(x = matrix(c(x1.teste, x2.teste), nc = 2), y = y.teste) -> loss_b

loss_b
```

Com o modelo realizado acima, consegui chegar no loss de `r loss_b`. Gostaria de deixar algumas observações: fiz diversos testes, fuições de ativação como sigmoide se mostraram ruins nesse modelo; não necessariamente muitas camadas fazem tanta diferença; batch size com potências de 2 funcionam muito bem; dropout não me deu bons resultados e deixou a função mais lenta; kernel não me deu bons resultados.

Segue os resultados:

## **Letra C**

**Considerando a rede ajustada no item b), responda os itens a seguir.**

**c)** Refaça o item h) da Lista 1 para essa nova rede. Comente os resultados.

```{r 2.c, error=FALSE,cache=TRUE}
  # Calcular os y's do modelo
predict(mod_b, matrix(c(x1.teste, x2.teste), nc = 2)) -> y_chapeu

  # Banco de dados para o gráfico
pred_obs <- data.frame(y_esp = teste$y, y_obs = y_chapeu)
colnames(pred_obs) <- c('y_esp','y_obs')
  
  # Gráfico
graph_c <- ggplot(pred_obs, aes(x = y_obs, y = y_esp )) + 
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(x = paste0("Valor esperado (ŷ)"), y = "Valor observado (y)") +
  theme_bw()

graph_c
```

Nota-se que a função está com um comportamento muito melhor do que no da lista 1. Sendo em vermelho a linha esperada para o 'mundo perfeito'. Nota-se também que há uma pequena variação que representa pouco mais que o desvio padrão da própria variável $Y$, já que $Y \sim N(\mu, \sigma = 1)$.

Ou seja, independente do modelo ainda haverá essa variação aleatória de, no mínimo, uma unidade.

## **Letra D**

**d)** Use a função de previsão do Keras para prever o valor da variável resposta $\hat{y} = f(x_1 = 1, x_2 = 1; \theta)$, para $\theta$ definido de acordo com a rede ajustada. (Veja o item a) da Lista 1).

```{r 2.d, error=FALSE,cache=TRUE}
predict(mod_b, matrix(c(1, 1), nc = 2)) -> y_predito

y_predito
```
## **Letra E**

**e)** Neste exemplo meramente didático, conhecemos a superfície que estamos estimando. Apresente, lado a lado, a Figura 1 da Lista 1 e a superfície estimada pela sua rede neural. Para tanto, basta trocar a variável mu pelos valores preditos pela rede. Comente os resultados.

Decidi alterar as cores do gráfico da superficie para manter o mesmo padrão da lista 1 que eu realizei.

```{r 2.e, error=FALSE,cache=TRUE}
  ### Figura 1: Gerando o gráfico da superfície
n <- 100
x1 <- seq(-3, 3, length.out=n)
x2 <- seq(-3, 3, length.out=n)
dados.grid <- as_tibble(expand.grid(x1, x2)) %>%
  rename_all(~ c("x1", "x2")) %>%
  mutate(mu=abs(x1^3 - 30*sin(x2) + 10))

ggplot(dados.grid, aes(x = x1, y = x2)) +
  geom_point(aes(colour = mu)) +
  scale_colour_gradient(low = "blue", high = "red",
                        name=TeX("$E(Y|X_1, X_2)$")) +
  labs(x = "x1", y = "x2", colour = "Resíduos")+ 
  ggtitle("Figura 1") -> antigo

  # Figura criada por mim
predict(mod_b, matrix(c(dados.grid$x1, dados.grid$x2), nc = 2)) -> y_keras

dados.grid$`y_keras` <- y_keras

ggplot(dados.grid, aes(x = x1, y = x2)) +
  geom_point(aes(colour = `y_keras`)) +
  scale_colour_gradient(low = "blue", high = "red",
                        name="Y previsto pelo modelo") +
  labs(x = "x1", y = "x2", colour = "Resíduos")+ 
  ggtitle("Gráfico da superficie estimada \n pela rede neural") -> novo

cowplot::plot_grid(antigo,novo)
```

Nota-se que as superfícies são muito parecidas, ou seja, o modelo está muito próximo do exemplo real, que é demonstrado pela figura 1.

Fiz tambem um gráfico de resíduos para mostrar a melhora comparado a lista 1.

```{r 2.ept2, error=FALSE,cache=TRUE}
dados_keras <- tibble(x1 = teste$x1.obs, x2 = teste$x2.obs,
                      y_chapeu = y_chapeu, y_real = teste$y) %>%
  mutate(residuos = y_real - y_chapeu)

  # Plotar gráfico de dispersão dos resíduos em função de x1 e x2
ggplot(dados_keras, aes(x = x1, y = x2)) +
  geom_point(aes(colour = residuos)) +
  scale_colour_gradient(low = "blue", high = "red",
                        name="Resíduos") +
  labs(x = "x1", y = "x2", colour = "Resíduos")+ 
  ggtitle("Gráfico de resíduos do modelo do keras")
```

Nota-se que há uma grande diferença entre o comportamento dos valores. No gráfico de resíduos produzido pelo modelo do keras, nota-se um valor de resíduos baixo, alem de que não possui padrão ou viés algum, como deve ser. 

## **Letra F**

**f)** Construa uma nova rede, agora ajustada sobre os valores previstos (ao invés dos valores observados de y) para cada observação dos conjuntos de treinamento e validação. Use a arquitetura mais parcimoniosa que conseguir, sem comprometer substancialmente o poder de previsão da rede (quando comparada à obtida no item 2b). Cite um possível uso para essa nova rede.

```{r 2.f, error=FALSE,cache=TRUE}
  # Calcular a predição para o banco de dados completo
y.pred = predict(mod_b, matrix(c(dados$x1.obs, dados$x2.obs), nc = 2))

  # Criar o banco de dados com x1 x2 e y.predito
dados.predito <- tibble(x1 = dados$x1.obs,
       x2 = dados$x2.obs,
       y.pred = y.pred) 

  # Separar o banco em treino e teste
# Treino
treino.pred <- dados.predito[1:80000,]
x1.treino.pred <- treino.pred$x1
x2.treino.pred <- treino.pred$x2
x.treino.pred <- treino.pred %>%
select(x1, x2)
y.treino.pred <- treino.pred$y.pred

# Teste
teste.pred <- dados.predito[80001:100000,]
x1.teste.pred <- teste.pred$x1
x2.teste.pred <- teste.pred$x2
x.teste.pred <- teste.pred %>%
select(x1, x2)
y.teste.pred <- teste.pred$y.pred
```

Agora vou criar o modelo e treiná-lo.

```{r 2.fpt1, error=FALSE,cache=TRUE, eval=FALSE}
  # Criando o modelo
mod_f <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = 2) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 8, activation = 'relu') %>%
  layer_dense(units = 1)

  # Taxa de aprendizagem e função de perda
tx_aprendizagem <- 0.0005

mod_f %>% 
  compile(optimizer = optimizer_adam(learning_rate = tx_aprendizagem),
          loss = "mse")

 # Treinamento do modelo
mod_f %>% 
  fit(x=matrix(c(x1.treino.pred, x2.treino.pred), nc=2),
      y=y.treino.pred,
      validation_data=list(matrix(c(x1.teste.pred, x2.teste.pred), nc=2),
                           y.teste.pred),
      callbacks=callback_early_stopping(
        monitor = "val_loss",
        patience = 25, 
        restore_best_weights = T),
      batch_size=2^5,
      epochs=500)
```


Segue os resultados:

```{r 2.fpt2, error=FALSE,cache=TRUE, message=TRUE}
  # Resultado do modelo no banco de dados com os y preditos
mod_f %>% 
  evaluate(x = matrix(c(x1.teste.pred, x2.teste.pred), nc = 2), y = y.teste.pred) -> loss_f
cat('Resultado do modelo no banco de dados de y preditos \n', loss_f)

  # Resultado do modelo no banco de dados com os y originais
mod_f %>% 
  evaluate(x = matrix(c(x1.teste, x2.teste), nc = 2), y = y.teste) -> new_loss
cat('Resultado do modelo no banco de dados de y originais \n', new_loss)
```
Percebe-se pelos resultados acima, que a performance desse modelo, apesar de não ter sido treinado no banco original, possui uma excelente performance no banco original, com um loss igual a `r new_loss`, sendo que o loss anterior, do modelo gerado na letra B, era igual a `r loss_b`.

Deve-se comentar tambem sobre a diminuição de $94\%$ de parâmetros (foram de $44.345$ para $2.945$), menos camadas, menos complexidade, performance mais ágil e mais interpretabilidade.

Essa nova rede neural ajustada sobre os valores previstos pode ser útil em várias situações, como:

- Modelagem preditiva hierárquica: Usar as previsões de um modelo prévio para melhorar o desempenho de um modelo subsequente.

- Aprendizado ativo: Utilizar as previsões do modelo para selecionar as amostras mais informativas para treinamento adicional.

- Construção de ensembles: Utilizar as previsões de diferentes modelos como entrada para uma nova rede neural que realiza uma combinação ou refinamento das previsões.